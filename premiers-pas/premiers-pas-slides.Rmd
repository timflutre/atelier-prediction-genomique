---
title: "Atelier prédiction génomique : <br> premiers pas"
author: "Vincent Segura (INRAE) <br><br> d'après ['premiers-pas.Rmd'](https://github.com/timflutre/atelier-prediction-genomique/raw/master/premiers-pas.Rmd) de Timothée Flutre (INRAE) <br>"
date: ""
output:
  ioslides_presentation:
    widescreen: yes
    incremental: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits=3)
```

# Introduction

## R, rmarkdown, RStudio

- Cette présentation a été générée à partir d'un fichier texte au format Rmd utilisé par le logiciel libre [R](http://www.r-project.org/)

- La fonction `render` du package [rmarkdown](https://cran.r-project.org/web/packages/rmarkdown/index.html) permet de générer le fichier html à partir du fichier Rmd

```{r rmarkdown, eval=FALSE}
library(rmarkdown)
render("premiers-pas-slides.Rmd")
```

- Il est généralement plus simple pour faire ça d'utiliser le logiciel [RStudio](http://www.rstudio.com/)

- Le format Rmd permet également d’utiliser le language LaTeX pour écrire des équations

## Packages

- Cette présentation nécessite par ailleurs le chargement des packages [MASS](https://cran.r-project.org/web/packages/MASS/index.html) et [stats4](https://www.rdocumentation.org/packages/stats4/versions/3.6.2) qui sont généralement inclus par defaut dans R

```{r MASS stats4}
library(MASS)
library(stats4)
```

## Notation et vocabulaire {.build}

<div>
- L'inférence avec un modèle statistique consiste généralement à **estimer les paramètres**, puis à s'en servir pour **prédire de nouvelles données**
</div>

<div>
- Lorsqu’on propose un modèle, on commence par expliquer les **notations**
- **Conventions** :
    * **lettres grecques** pour les **paramètres** (non-observés), par exemple $\theta$
    * **lettres romaines** pour les **données observées**, $y$
    * **lettres romaines** surmontées d'un **tilde** pour les **données prédites**, $\tilde{y}$
    * les **ensembles** de données ou de paramètres sont généralement notés en **majuscule**, $\mathcal{D} = \{ y_1, y_2, y_3 \}$ ou $\Theta = \{ \theta_1, \theta_2 \}$
    * s'il y a plusieurs paramètres ou données, ils se retrouvent mathématiquement dans des **vecteurs**, en **gras**, $\boldsymbol{\theta}$ et $\boldsymbol{y}$
    * les vecteurs sont en **colonne**
</div>

## La notion de vraisemblance {.build}

<div>
- Une fois les notations établies, on écrit la **vraisemblance** (*likelihood*), souvent présentée comme étant la *"probabilité des données sachant les paramètres"*

- Si les données sont des **variables continues**, c'est la densité de probabilité des données sachant les paramètres, notée $p(y | \theta)$
<!-- , si les données sont des **variables discrètes**, c'est la fonction de masse, notée $P(y | \theta)$ -->

- La vraisemblance est une fonction des **paramètres**, d'où le fait qu'on la note $\mathcal{L}(\theta)$ ou $\mathcal{L}(\theta | y)$
</div>

<div>
- la méthode du **maximum de vraisemblance** cherche à identifier la valeur du paramètre, notée $\hat{\theta}$ par convention, qui maximise la vraisemblance <br>
$$
\begin{align}
\hat{\theta} = \text{argmax}_\theta \, \mathcal{L} \; \; \Leftrightarrow \; \; \frac{\partial \mathcal{L}}{\partial \theta}(\hat{\theta}) = 0
\end{align}
$$
</div>

<!-- ## L'incertitude -->

<!-- - Certains décrivent les statistiques comme étant la "science de l'incertitude" ([Lindley, 2000](http://dx.doi.org/10.1111/1467-9884.00238)) -->

<!-- - Pour l'instant, nous n'avons parlé que de la façon d'obtenir *une* valeur par paramètre, celle qui maximise la vraisemblance. -->

<!-- - Il est donc primordial ensuite de **quantifier l'incertitude** que nous avons quant à cette valeur -->

<!-- - C'est sur ce point que différentes approches sont possibles (**fréquentiste**, **bayésienne**). -->

## Comprendre la vraisemblance {.build}

<div>
- Supposons que l'on étudie une quantité physique dont la valeur résulte de la **somme** d'une **très grande quantité** de **facteurs indépendants**, **chacun ayant un faible impact** sur la valeur finale

- On prend trois mesures de cette quantité d'intérêt
</div>

<div>
- Comme il y a de la variation, on choisit d'introduire une **variable aléatoire** $Y$ correspondant à la quantité d'intérêt, et on dénote par $y_1$, $y_2$ et $y_3$ les trois observations, vues comme des réalisations de cette variable aléatoire : 
```{r echo=FALSE}
set.seed(1)
y <- rnorm(n = 3, mean = 5, sd = 1)
```
<div class="centered">
$y_1$ = `r y[1]`, $y_2$ = `r y[2]`, $y_3$ = `r y[3]`
</div>
</div>

<div>
- Etant donné les caractéristiques du phénomène, il est raisonnable de supposer que la variable $Y$ suit une **loi Normale** (c.f. le [théorème central limite](https://fr.wikipedia.org/wiki/Th%C3%A9or%C3%A8me_central_limite)
</div>

----

- Cette distribution de probabilité est caractérisée par deux paramètres, sa **moyenne** que l'on note généralement $\mu$, et sa **variance** que l'on note généralement $\sigma^2$ ($\sigma$ étant l'écart-type)

- En terme de notation, on écrit $Y \sim \mathcal{N}(\mu, \sigma^2)$, et la densité de probabilité de la réalisation $y$ de $Y$ s'écrit :
$$
\begin{align}
Y \sim \mathcal{N}(\mu, \sigma^2) \; \; \Leftrightarrow \; \; p(Y=y \, | \, \mu, \sigma) = \frac{1}{\sigma \sqrt{2 \pi}} \exp \left( - \frac{(y - \mu)^2}{2 \sigma^2} \right)
\end{align}
$$

- L'intérêt de ce **modèle paramétrique** est de pouvoir **"résumer"** les données, par exemple un million de mesures, par seulement **2 valeurs**, les **paramètres**

----

- Mais, nous ne connaissons pas les valeurs de paramètres !

- La moyenne $\mu$ peut prendre toutes les valeurs entre $-\infty$ et $+\infty$, et la variance $\sigma^2$ n'a pour seule restriction que d'être positive

- La loi Normale peut être assez différente selon les valeurs de ces paramètres

<div style="position: relative; top: -30px">
```{r effect_mu effect_sigma, echo=FALSE, dev.args = list(bg = 'transparent'), fig.height=4.5, fig.width=9.5, fig.align='center'}
par(mfrow = c(1, 2))
x <- seq(from=-3, to=10, length.out=500)
plot(x=x, y=dnorm(x=x, mean=2, sd=1), type="l", col="blue", las=1,
     main=expression(bold(Comparaison~de~deux~lois~Normales~(sigma==1))),
     xlab="valeur de la variable aléatoire, y",
     ylab="densité de probabilité, p(y)")
points(x=x, y=dnorm(x=x, mean=5, sd=1), type="l", col="red")
abline(v=0, lty=2)
legend("topright", legend=c(expression(mu==2), expression(mu==5)),
       col=c("blue", "red"), text.col=c("blue", "red"), lty=1, bty="n")
x <- seq(from=-7, to=13, length.out=500)
plot(x=x, y=dnorm(x=x, mean=3, sd=1), type="l", col="blue", las=1,
     main=expression(bold(Comparaison~de~deux~lois~Normales~(mu==3))),
     xlab="valeur de la variable aléatoire, y",
     ylab="densité de probabilité, p(y)")
points(x=x, y=dnorm(x=x, mean=3, sd=3), type="l", col="red")
abline(v=0, lty=2)
legend("topright", legend=c(expression(sigma==1), expression(sigma==3)),
       col=c("blue", "red"), text.col=c("blue", "red"), lty=1, bty="n")
```
</div>

----

<div style="position: relative; top: -10px">
- Revenons à nos trois mesures : `r y`
</div>

<div style="position: relative; top: -20px">
- Parmi toutes les valeurs possibles des paramètres, quelles sont celles pour lesquelles la loi Normale est une bonne description du mécanisme qui a généré ces données ?
</div>

<div style="position: relative; top: -30px">
- Pour simplifier, supposons que l'on connaisse déjà la variance : $\sigma^2 = 1$,<br>il ne nous reste plus qu'à trouver la moyenne : $\mu$.
</div>

<div style="position: relative; top: -40px">
- Pour la première observation, $y_1$ = `r y[1]` :
</div>

<div style="position: relative; top: -60px">
```{r ex_lik_y1, echo=FALSE, dev.args = list(bg = 'transparent'), fig.height=3.4, fig.width=6.1, fig.align='center'}
par(mar = c(4, 4, 2, 1.5))
x <- seq(from=-3, to=10, length.out=500)
plot(x=x, y=dnorm(x=x, mean=2, sd=1), type="l", col="blue", las=1,
     main=expression(bold(Comparaison~de~deux~vraisemblances~Normales~(sigma==1))),
     xlab="valeur de la variable aléatoire, y",
     ylab="densité de probabilité, p(y)")
points(x=x, y=dnorm(x=x, mean=5, sd=1), type="l", col="red")
abline(v=0, lty=2)
legend("topright", legend=c(expression(mu==2), expression(mu==5)),
       col=c("blue", "red"), text.col=c("blue", "red"), lty=1, bty="n")
segments(x0=y[1], y0=0, x1=y[1], y1=dnorm(x=y[1], mean=2, sd=1))
segments(x0=y[1], y0=0, x1=y[1], y1=dnorm(x=y[1], mean=5, sd=1))
segments(x0=y[1], y0=dnorm(x=y[1], mean=2, sd=1),
         x1=-5, y1=dnorm(x=y[1], mean=2, sd=1), col="blue")
segments(x0=y[1], y0=dnorm(x=y[1], mean=5, sd=1),
         x1=-5, y1=dnorm(x=y[1], mean=5, sd=1), col="red")
text(x=1.1*y[1], y=0.18, labels=expression(y[1]))
text(x=-1.6, y=dnorm(x=y[1], mean=2, sd=1), col="blue", pos=3,
     labels=expression(paste("p(",y[1]," | ",mu==2,")")))
text(x=-1.6, y=dnorm(x=y[1], mean=5, sd=1), col="red", pos=3,
     labels=expression(paste("p(",y[1]," | ",mu==5,")")))
```
</div>

----

- D'après le graphique précédent : $p(y_1 \, | \, \mu=5, \sigma=1) \, > \, p(y_1 \, | \, \mu=2, \sigma=1)$

- Cela se vérifie si l'on fait le calcul avec la formule :
    * $p(y_1 \, | \, \mu=5, \sigma=1)$ = `r dnorm(x=y[1], mean=5, sd=1)`
    * $p(y_1 \, | \, \mu=2, \sigma=1)$ = `r dnorm(x=y[1], mean=2, sd=1)`

<!-- - Comme les deux densités ont la même valeur pour $\sigma$, la différence vient bien du terme $(y - \mu)^2$ dans l'exponentielle, terme qui représente l'écart à la moyenne -->

- Au final, nous pouvons conclure pour la première observation, que la vraisemblance $\mathcal{L}(\mu=5,\sigma=1)$ est plus grande que $\mathcal{L}(\mu=2,\sigma=1)$

----

- Comme on dispose de **plusieurs observations**, $\{y_1,y_2,y_3\}$, et qu'on suppose qu'elles sont toutes des réalisations de la même variable aléatoire, $Y$, il est pertinent de calculer la **vraisemblance** de toutes ces observations **conjointement** plutôt que séparément :
$$
\begin{align}
\mathcal{L}(\mu, \sigma) = p(y_1,y_2,y_3 \, | \, \mu, \sigma)
\end{align}
$$

- Si l'on fait aussi l'hypothèse que ces observations sont **indépendantes**, cela se simplifie en :
$$
\begin{align}
\mathcal{L}(\mu, \sigma) &= p(y_1 \, | \, \mu, \sigma) \times p(y_2 \, | \, \mu, \sigma) \times p(y_3 \, | \, \mu, \sigma) \\
&= \prod_{i=1}^3 p(y_i \, | \, \mu, \sigma)
\end{align}
$$

----

- Il n'est pas très pratique de maximiser la vraisemblance directement, on préfère passer au log (qui est monotone, donc le maximum de l'un est aussi le maximum de l'autre) :
$$
\begin{align}
l(\mu,\sigma) &= \log \mathcal{L}(\mu, \sigma) \\
&= \sum_{i=1}^3 \log p(y_i \, | \, \mu, \sigma) \\
&= \sum_{i=1}^3 \log \left[ \frac{1}{\sigma \sqrt{2 \pi}} \exp \left( - \frac{(y_i - \mu)^2}{2 \sigma^2} \right) \right] \\
&= - 3 \log \sigma \; - \frac{3}{2} \log (2\pi) \; - \frac{1}{2 \sigma^2} \sum_{i=1}^3 (y_i - \mu)^2
\end{align}
$$

----

- En pratique, on écrit une **fonction** qui calcule la **log-vraisemblance**, et on cherche le **maximum** de cette fonction

```{r compute_lik}
compute.log.likelihood <- function(parameters, data){
  mu <- parameters[1]
  sigma <- parameters[2]
  y <- data
  n <- length(y)
  log.lik <- - n * log(sigma) - (n/2) * log(2 * pi) - sum(((y - mu)^2) / (2 * sigma^2))
  return(log.lik)
}

compute.log.likelihood(c(5,1), y)
compute.log.likelihood(c(2,1), y)
```

----

- Dans le cas de la **loi Normale**, il existe déjà dans R des fonctions implémentant la densité de probabilité, ce qui nous permet de vérifier que nous n'avons pas fait d'erreur

```{r check_compute_lik}
sum(dnorm(x=y, mean=5, sd=1, log=TRUE))
sum(dnorm(x=y, mean=2, sd=1, log=TRUE))
```

# Ecrire le modèle

## Notations

<div style="position: relative; top: -20px">
- $n$ : nombre d'individus (diploïdes, supposés non-apparentés)
</div>

<div style="position: relative; top: -25px">
- $i$ : indice indiquant le $i$-ème individu, $i \in \{1,\ldots,n\}$
</div>

<div style="position: relative; top: -30px">
- $y_i$ : phénotype de l'individu $i$ pour la caractère d'intérêt
</div>

<div style="position: relative; top: -35px">
- $\mu$ : moyenne globale du phénotype des $n$ individus
</div>

<div style="position: relative; top: -40px">
- $f$ : fréquence de l'allèle minoritaire au marqueur SNP d'intérêt
</div>

<div style="position: relative; top: -45px">
- $x_i$ : génotype de l'individu $i$ à ce SNP, codé comme le nombre de copie(s) de l'allèle minoritaire, $\forall i \; x_i \in \{0,1,2\}$
</div>

<div style="position: relative; top: -50px">
- $\beta$ : effet additif de chaque copie de l'allèle minoritaire en unité du phénotype
</div>

## Notations (suite)

- $\epsilon_i$ : erreur pour l'individu $i$

- $\sigma^2$ : variance des erreurs

- Données : $\mathcal{D} = \{ (y_1 \, | \, x_1), \ldots, (y_n \, | \, x_n) \}$

- Paramètres : $\Theta = \{ \mu, \beta, \sigma \}$

## Vraisemblance

- On suppose que le génotype au SNP d'intérêt a un effet additif sur la moyenne du phénotype, ce qui s'écrit généralement :
$$
\begin{align}
\forall i \; \; y_i = \mu + \beta x_i + \epsilon_i \text{ avec } \epsilon_i \overset{\text{i.i.d}}{\sim} \mathcal{N}(0, \sigma^2)
\end{align}
$$

- Une autre façon équivalente de l'écrire :
$$
\begin{align}
\forall i \; \; y_i \, | \, x_i, \mu, \beta, \sigma \; \overset{\text{i.i.d}}{\sim} \mathcal{N}(\mu + \beta x_i, \sigma^2)
\end{align}
$$

# Simuler des données

----

- **Initialisation** :<br><br>
On utilise un générateur de nombres pseudo-aléatoires qui peut être initialisé avec une graine (seed), ce qui est très utile pour la reproductibilité des analyses

```{r set_seed}
set.seed(1866) # année de parution de l'article de Mendel fondant la génétique
```

- **Nombre d'individus** :

```{r set_sample_size}
n <- 200
```

- **Moyenne générale** :

```{r set_global_mean}
mu <- 50
```

----

- **Génotypes** (on suppose que la population est à l'équilibre d'Hardy-Weinberg) :

```{r simul_geno}
##' Genotype frequencies
##'
##' Calculate the genotype frequencies at a locus assuming the Hardy-Weinberg equilibrium
##' (https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle).
##' @param maf frequency of the minor allele, a
##' @return vector of genotype frequencies
##' @author Timothee Flutre
calcGenoFreq <- function(maf){
  stopifnot(is.numeric(maf), length(maf) == 1, maf >= 0, maf <= 0.5)
  geno.freq <- c((1 - maf)^2,
                2 * (1 - maf) * maf,
                maf^2)
  names(geno.freq) <- c("AA", "Aa", "aa")
  return(geno.freq)
}
f <- 0.3
genotypes <- sample(x=c(0,1,2), size=n, replace=TRUE, prob=calcGenoFreq(f))
```

----

```{r look_geno}
head(genotypes)
table(genotypes)
sum(genotypes) / (2 * n) # estimate of the MAF
var(genotypes) # important for the estimate of beta
```

----

- **Effet du génotype sur le phénotype**, $\beta$ :

```{r simul_geno_effect}
(beta <- rnorm(n=1, mean=2, sd=1))
```

- **Erreurs**, $\boldsymbol{\epsilon}$ (par simplicité, on fixe $\sigma$ à 1) :
```{r simul_errors}
sigma <- 1
errors <- rnorm(n=n, mean=0, sd=sigma)
```

----

- Nous avons maintenant tout ce qu'il faut pour **simuler les phénotypes**, $\boldsymbol{y}$, via l'**équation** précédente : $y_i = \mu + \beta x_i + \epsilon_i$

```{r simul_pheno}
phenotypes <- mu + beta * genotypes + errors
```

- Il est habituel dans R d'organiser les données dans un **tableau**

```{r org_data}
dat <- data.frame(x=genotypes, y=phenotypes)
head(dat)
```

# Réaliser l'inférence

## Visualisation graphique

- **Distribution** du phénotype

```{r look_pheno, dev.args = list(bg = 'transparent'), fig.height=3.5, fig.width=4, fig.align='center'}
par(mar = c(4, 4, 1, 1))
hist(phenotypes, las = 1, main = "",
     xlab = expression(paste("Phénotypes, ", bold(y))),
     ylab = "Nombre d'individus")
```

----

- **Relation génotypes - phénotypes**

```{r look_geno_pheno, dev.args = list(bg = 'transparent'), fig.height=3.5, fig.width=4.5, fig.align='center'}
par(mar = c(4, 4, 1, 1))
boxplot(phenotypes ~ genotypes,
        xlab = expression(paste("Génotypes, ", bold(x))),
        ylab = expression(paste("Phénotypes, ", bold(y))),
        varwidth = TRUE, notch = TRUE, las = 1, xaxt = "n", at = 0:2)
axis(side = 1, at = 0:2, labels = c("0 (AA)", "1 (Aa)", "2 (aa)"))
```

## Implémentation (facile)

- Sous R, la fonction `lm` implémente l'estimation par maximum de vraisemblance

```{r fit_lm}
fit <- lm(y ~ x, data=dat)
```

- Vérification des hypothèses du modèle (homoscédasticité, normalité, indépendance)

```{r diagnostics, fig.show='hide'}
par(mfrow=c(2, 2), mar = c(4, 4, 2, 1))
plot(fit)
```

----

```{r diagnostics plot, echo = FALSE, dev.args = list(bg = 'transparent'), fig.height=6, fig.width=6, fig.align='center'}
par(mfrow=c(2, 2), mar = c(4, 4, 2, 1))
plot(fit)
```

----

```{r get_results_summary}
summary(fit)
```

----

- Représentation graphique du modèle

```{r look_geno_pheno_reg code, fig.show='hide'}
par(mar = c(4, 4, 1, 1))
boxplot(phenotypes ~ genotypes,
        xlab=expression(paste("Génotypes, ", bold(x))),
        ylab=expression(paste("Phénotypes, ", bold(y))),
        varwidth=TRUE, notch=TRUE, las=1, xaxt="n", at=0:2)
axis(side=1, at=0:2, labels=c("0 (AA)", "1 (Aa)", "2 (aa)"))
abline(a=coefficients(fit)[1], b=coefficients(fit)[2], col="red")
legend("bottomright",
       legend=expression(hat(y)[i]==hat(mu)~+~hat(beta)~x[i]),
       col="red", lty=1, bty="n")
```

----

```{r look_geno_pheno_reg, echo = FALSE, dev.args = list(bg = 'transparent'), fig.height=5.5, fig.width=6.5, fig.align='center'}
par(mar = c(4, 4, 1, 1))
boxplot(phenotypes ~ genotypes,
        xlab=expression(paste("Génotypes, ", bold(x))),
        ylab=expression(paste("Phénotypes, ", bold(y))),
        varwidth=TRUE, notch=TRUE, las=1, xaxt="n", at=0:2)
axis(side=1, at=0:2, labels=c("0 (AA)", "1 (Aa)", "2 (aa)"))
abline(a=coefficients(fit)[1], b=coefficients(fit)[2], col="red")
legend("bottomright",
       legend=expression(hat(y)[i]==hat(mu)~+~hat(beta)~x[i]),
       col="red", lty=1, bty="n")
```

## Implémentation (plus difficile)

- Il faut d'abord écrire une fonction calculant l'opposé de la log-vraisemblance

```{r neg_log_lik}
negLogLik <- function(mu, beta, sigma){
  - sum(dnorm(x=dat$y, mean=mu + beta * dat$x, sd=sigma, log=TRUE))
}
```

- Puis demander à la fonction `mle` de la maximiser (en spécifiant que le paramètre $\sigma$ ne peut pas être négatif ou nul)

```{r fit_mle}
fit2 <- mle(negLogLik, start=list(mu=mean(dat$y), beta=0, sigma=1),
            method="L-BFGS-B", nobs=nrow(dat),
            lower=c(-Inf,-Inf,10^(-6)),
            upper=c(+Inf,+Inf,+Inf))
```

----

```{r summary fit_mle}
summary(fit2)
```

# Evaluer les résultats

## Sélection de modèles

- Evaluation de l'ajustement du modèle aux données

- Dans notre cas de régression linéaire simple, on peut utiliser le coefficient de détermination $R^2$

```{r adjust}
summary(fit)$r.squared
```

----

- On peut facilement vérifier que cette valeur renvoyée par la fonction `lm` correspond à la formule :

$$
\begin{align}
R^2 = \frac{\hat{\beta}^2 \; Var(\boldsymbol{x})}{\hat{\beta}^2 \; Var(\boldsymbol{x}) + \hat{\sigma}^2}
\end{align}`
$$
```{r calc_adjust}
(coefficients(fit)[2]^2 * var(dat$x)) /
  (coefficients(fit)[2]^2 * var(dat$x) + summary(fit)$sigma^2)
```

## Estimation des paramètres

- Prenons l'exemple de $\beta$, comme nous avons simulé les données, nous connaissons sa vraie valeur

```{r true_beta}
beta
```

- Après avoir ajusté le modèle avec la fonction `lm`, nous pouvons récupérer l'estimation de ce paramètre ($\hat{\beta}$)

```{r estim_beta}
(beta.hat <- coefficients(fit)[2])
```

----

- Pour comparer les deux, on définit une **fonction de perte** (*loss function*) reliant le **paramètre** ($\beta$) à **son estimation** ($\hat{\beta}$)

- On utilise une fonction quadratique, dont on prend l'espérance, ce qui donne l'**erreur quadratique moyenne** (*mean squared error*)

$$
\begin{align}
MSE = E \left( (\hat{\beta} - \beta)^2 \right)
\end{align}
$$

- On calcule sa racine carrée pour que le résultat soit dans la même unité que le paramètre

```{r rmse_beta}
(rmse.beta <- sqrt((beta.hat - beta)^2))
```

## Prédiction de données

- On peut aussi calculer l'**erreur quadratique moyenne** avec les **phénotypes** déjà **observés** (on parle de *in-sample predictions*)

```{r calc_rmse_y}
y <- phenotypes
y.hat <- (coefficients(fit)[1] + coefficients(fit)[2] * genotypes)
errors <- y - y.hat
(rmse.y <- sqrt(mean(errors^2)))
```

- On peut aussi utiliser la fonction `predict`

```{r rmse_y}
errors <- phenotypes - predict(fit)
(rmse.y <- sqrt(mean(errors^2)))
```

----

- Le vecteur *errors* correspond aux **résidus** du modèle

```{r resid}
head(errors)
head(resid(fit))
(rmse.y <- sqrt(mean(resid(fit)^2)))
```

----

- De façon plus intéressante, on souhaiterait évaluer les **prédictions** phénotypiques sur $n_{\text{new}}$ **nouveaux individus**

- Pour cela, on commence par simuler de nouvelles données, $\mathcal{D}_{\text{new}} = \{(y_{i,\text{new}} \, | \, x_{i,\text{new}})\}$, toujours avec les *mêmes* "vraies" valeurs des paramètres, $\Theta = \{\mu, \beta, \sigma\}$

```{r simul_new_data}
set.seed(1944) # année de découverte de l'ADN comme support des gènes
n.new <- 100
x.new <- sample(x=c(0,1,2), size=n.new, replace=TRUE, prob=calcGenoFreq(f))
y.new <- mu + beta * x.new + rnorm(n=n.new, mean=0, sd=sigma)
```

----

- Puis on utilise les **estimations** des paramètres obtenues précédemment pour **prédire** les **nouveaux phénotypes** à partir des **nouveaux génotypes**, $\tilde{\mathcal{D}}_{\text{new}} = \{(\tilde{y}_{i,\text{new}} = \hat{\mu} + \hat{\beta} \, x_{i,\text{new}})\}$ (*out-of-sample predictions*)

```{r predict_ytilde}
y.new.tilde <- (coefficients(fit)[1] + coefficients(fit)[2] * x.new)
```

- Enfin, on calcule l'**erreur quadratique moyenne** de prédiction

```{r rmspe}
errors.tilde <- y.new - y.new.tilde
(rmspe <- sqrt(mean(errors.tilde^2)))
```

----

Graphiquement :

```{r ynew_vs_ytilde, dev.args = list(bg = 'transparent'), fig.height=3.5, fig.width=4, fig.align='center'}
par(mar = c(4, 4.5, 1, 1))
plot(x=y.new, y=y.new.tilde, las=1,
     xlab=expression(paste("Nouveaux vrais phénotypes, ", bold(y)[new])),
     ylab=expression(paste("Nouveaux phénotypes prédits, ", bold(tilde(y))[new])))
abline(a=0, b=1, lty=2)
legend("topleft", legend="ligne identité", lty=2, bty="n")
```

# Perspectives

## Explorer les simulations possibles

- La simulation est un outil particulièrement utile pour explorer **comment un modèle répond** à des **changements** dans les **données** et les **paramètres**

- On pourrait par exemple avoir envie de savoir ce qui se passe si la taille de l'échantillon ($n$) varie

- Idem, que se passe-t-il si, à $n$ et $\sigma$ fixés, on modifie $\beta$ ?

<!-- <div class="centered" style="font-size:40px; position: relative; top: 20px"> -->
<!-- **C'est à vous de jouer !** -->
<!-- </div> -->

<!-- ## Ré-écrire le modèle -->

<!-- - Modèle sous forme *multivariée* -->
<!-- $$ -->
<!-- \begin{align} -->
<!-- \boldsymbol{y} \, | \, \boldsymbol{x}, \mu, \beta, \sigma \; \sim \mathcal{N}_n(\boldsymbol{1} \mu + \boldsymbol{x} \beta, \sigma^2 I_n) -->
<!-- \end{align} -->
<!-- $$ -->
<!-- - avec : -->
<!--     * $\boldsymbol{y}$ : vecteur de dimension $n$ contenant les phénotypes, -->
<!--     * $\boldsymbol{x}$ : vecteur de dimension $n$ contenant les génotypes, -->
<!--     * $\boldsymbol{1}$ : vecteur de dimension $n$ ne contenant que des $1$, -->
<!--     * $I_n$ : matrice identité de dimension $n \times n$ -->

<!-- ---- -->

<!-- - Exemple d'un vecteur aléatoire de longueur 2, $\boldsymbol{\theta}$, distribué selon une loi Normale bivariée $\mathcal{N}_2(\boldsymbol{\mu}, \Sigma)$ : -->
<!-- $$ -->
<!-- \boldsymbol{\theta} \sim \mathcal{N}_2(\boldsymbol{\mu}, \Sigma) -->
<!-- \Leftrightarrow -->
<!-- \begin{align} -->
<!-- \begin{bmatrix} -->
<!-- \theta_1 \\ -->
<!-- \theta_2 -->
<!-- \end{bmatrix} -->
<!-- \sim -->
<!-- \mathcal{N}_2 \left( -->
<!-- \begin{bmatrix} -->
<!-- \mu_1 \\ -->
<!-- \mu_2 -->
<!-- \end{bmatrix}, -->
<!-- \begin{bmatrix} -->
<!-- \sigma_1^2 & \sigma_{12}^2 \\ -->
<!-- \sigma_{12}^2 & \sigma_2^2 -->
<!-- \end{bmatrix} -->
<!-- \right) -->
<!-- \end{align} -->
<!-- $$ -->

<!-- ---- -->

<!-- - On fixe quelques valeurs -->

<!-- ```{r norm_bivar_param} -->
<!-- mu.1 <- 5 -->
<!-- mu.2 <- 23 -->
<!-- mu <- c(mu.1, mu.2) -->
<!-- var.1 <- 1 -->
<!-- var.2 <- 2 -->
<!-- rho <- 0.8 -->
<!-- covar.12 <- rho * sqrt(var.1 * var.2) -->
<!-- Sigma <- matrix(c(var.1, covar.12, covar.12, var.2), nrow=2, ncol=2) -->
<!-- ``` -->

<!-- ---- -->

<!-- - On simule des vecteurs aléatoires avec la fonction `mvrnorm` du package [MASS](https://cran.r-project.org/web/packages/MASS/index.html) -->

<!-- ```{r norm_bivar_simul} -->
<!-- theta <- mvrnorm(n=100, mu=mu, Sigma=Sigma) -->
<!-- dim(theta) -->
<!-- head(theta) -->
<!-- ``` -->

<!-- ---- -->

<!-- - Représentation graphique -->

<!-- ```{r norm_bivar_plots, dev.args = list(bg = 'transparent'), fig.height=4, fig.width=5.5, fig.align='center'} -->
<!-- bivn.kde <- kde2d(x=theta[,1], y=theta[,2], n=50) -->
<!-- image(bivn.kde, xlab=expression(theta[1]), ylab=expression(theta[2]), las=1, -->
<!--       main=bquote(bold(paste("Echantillons d'une Normal bivariée avec ", -->
<!--                              rho, " = ", .(rho))))) -->
<!-- contour(bivn.kde, add=T) -->
<!-- ``` -->

# Annexe

----

<div style="position: relative; top: -50px">
```{r info}
print(sessionInfo(), locale=FALSE)
```
</div>
